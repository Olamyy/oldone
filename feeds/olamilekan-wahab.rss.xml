<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Indent</title><link>/</link><description></description><lastBuildDate>Sat, 05 May 2018 00:00:00 +0000</lastBuildDate><item><title>The Sklearn API Design Principles</title><link>/blog/2018/05/05/understanding-the-sklearn-design/</link><description>&lt;p&gt;&lt;strong&gt;Hi&lt;/strong&gt;, &lt;/p&gt;
&lt;p&gt;You should really read this to the end if:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You've never really understood how sklearn modules/classes are structured.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You're interested in contributing to sklearn.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You need to understand the internals of sklearn, why some decisions were made and how some things work.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Some weeks back, I decided to start contributing to the open source scikit-learn library. This decision led me to read a whole lot of sklearn's code, documentation , contribution and 
online guides.&lt;/p&gt;
&lt;p&gt;This article is an attempt to document everything I learnt about the design principles behind the library.&lt;/p&gt;
&lt;p&gt;The rest of the article will be organised as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is a scikit in python? &lt;ul&gt;
&lt;li&gt;Examples of scikits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scikit-learn&lt;ul&gt;
&lt;li&gt;What is scikit-learn?&lt;/li&gt;
&lt;li&gt;A little history&lt;/li&gt;
&lt;li&gt;Data representation&lt;/li&gt;
&lt;li&gt;API Design Principles&lt;ul&gt;
&lt;li&gt;Consistency&lt;ul&gt;
&lt;li&gt;Estimators&lt;/li&gt;
&lt;li&gt;Predictors&lt;/li&gt;
&lt;li&gt;Transformers&lt;/li&gt;
&lt;li&gt;Meta-estimators&lt;/li&gt;
&lt;li&gt;Pipelines and Feature Unions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other Principles&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Conclusion&lt;/li&gt;
&lt;li&gt;Resources  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;What is a scikit in python?&lt;/h2&gt;
&lt;p&gt;A scikit, short for SciPy Toolkit(or the more expressive, Scientifica Python Toolkit) is any open sourced software written in Python to supplement Scipy. Scikits are essentially add on 
packages 
that are related to either science, engineering or research in one field or another. They're usually BSD licensed and are built upon existing libraries like scipy and numpy. A package can be called
a scikit when :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It is too specialized on a field or topic to part of SciPy.&lt;/li&gt;
&lt;li&gt;It is GPL licensed (This makes it incompatible with SciPyâ€™s BSD license).&lt;/li&gt;
&lt;li&gt;It's meant to be part of Scipy but is still actively being developed.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Example of scikits&lt;/h3&gt;
&lt;p&gt;Some of the most widly used example scikit packages are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Machine Learning Specific&lt;ul&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/"&gt;Scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-neuralnetwork"&gt;Scikit-neuralnetwork&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-splearn"&gt;scikit-splearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-metrics"&gt;scikit-metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-tensor"&gt;scikit-tensor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-surprise"&gt;scikit-surprise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-stack"&gt;scikit-stack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-graph"&gt;scikit-graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-keras"&gt;scikit-keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Media Specific Scikits&lt;ul&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-image"&gt;Scikit-image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-sound"&gt;Scikit-sound&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-video"&gt;Scikit-video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-xray"&gt;Scikit-xray&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data Specific&lt;ul&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-dataaccess"&gt;Scikit-dataacces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/pandas/pandas-dev"&gt;Pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-data"&gt;Scikit-data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-datasets"&gt;Scikit-datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-discovery"&gt;Scikit-discovery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Visualization Specific Scikits&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;Matplotlib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-visualizations"&gt;Scikit-visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-vi"&gt;Scikit-vi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-vis"&gt;Scikit-vis&lt;/a&gt;   &lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-viz"&gt;Scikit-viz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikits.appspot.com/scikit-plot"&gt;Scikit-plot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While the list above covers some popular scikits, the official &lt;a href="http://scikits.appspot.com/scikits"&gt;scikit appspot website&lt;/a&gt; links to a whole lot of other scikits.&lt;/p&gt;
&lt;h2&gt;Scikit-learn&lt;/h2&gt;
&lt;h3&gt;What is scikit-learn?&lt;/h3&gt;
&lt;p&gt;Scikit-learn(shortened as sklearn) is an open source library that provides a consistent API for using traditional state of the art Machine Learning algorithms/methods in Python.&lt;/p&gt;
&lt;p&gt;Majorly written in Python, some of sklearn's internal algorithms are written in Cython using third party bindings like LIBSVM and 
LIBLINEAR. Sklearn's major python dependencies are scipy, numpy and matplotlib. &lt;/p&gt;
&lt;p&gt;&lt;img alt="sklearn dependency tree." src="/images/sklearn.png" /&gt; &lt;/p&gt;
&lt;p&gt;Figure 1 : This shows a dependency tree for the sklearn library.&lt;/p&gt;
&lt;h2&gt;A little history&lt;/h2&gt;
&lt;p&gt;During the Google Summer of Code in 2007, David Cournapeau decided to put together a number of algorithms he has been using over time into a python module. A while after this, Matthieu Brucher joined
the project and decided to use it for computational work in his thesis. This gave sklearn more light and in no time(2010 to be specific), &lt;a href=""&gt;INRA&lt;/a&gt; got involved in it and the first major release was 
published in the same year.&lt;/p&gt;
&lt;h2&gt;Data Representation&lt;/h2&gt;
&lt;p&gt;Machine learning data in different fields, languages, etc. is generally represented as a pair of matrices with numerical values, say $X$ and $y$. The $X$ variables often represent the input values and
the $y$ variables represent the output values. Each row of these matrices  to one sample of the dataset and each column to one variable of the problem.
Sklearn as expected also follows the same form for representing data internally. To represent data, it classifies all data under 3 types of data:&lt;/p&gt;
&lt;h4&gt;1. Sparse Data&lt;/h4&gt;
&lt;p&gt;A sparse data is any dataset that mostly contains more zeros than non-zero entries. To be a bit technical, a vector is $k$-sparse if it contains at most $k$ nonzero entries. &lt;/p&gt;
&lt;p&gt;&lt;img alt="sparse matrix." src="/images/sparse_matrix.png" /&gt; &lt;/p&gt;
&lt;p&gt;Figure 2 : An example of a sparse matrix.&lt;/p&gt;
&lt;p&gt;Sklearn represents sparse data as scipy sparse matrices. It does this because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Storing all zero values in a sparse matrix is a computational waste.&lt;/li&gt;
&lt;li&gt;It is much more efficient to operate only on elements that will return nonzero values.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To avoid these, sklearn uses a scipy algorithm called Compressed Sparse Row (CSR). This essentially compresses the memory footprint of the matrix object and speeds up insert and retrieval processes
 to optimize the operation of many sklearn's algorithms. &lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Sklearn has a number of algorithms that support sparse matrix operations. If you ever need to check if an algorithm supports sparse data, check the documentation of the  &lt;code&gt;fit&lt;/code&gt; method of it's 
 class. If it has an &lt;code&gt;X: {array-like, sparse matrix}&lt;/code&gt; entry, then it supports sparse data entry.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a much more thorough understanding of how scipy sparse matrices work, you can check out &lt;a href="https://github.com/dziganto/Data_Science_Fundamentals"&gt;David Ziganto's&lt;/a&gt; article &lt;a href="https://dziganto.github.io/Sparse-Matrices-For-Efficient-Machine-Learning/"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;2. Dense Data&lt;/h4&gt;
&lt;p&gt;A dense data contains mostly non-zero entries. &lt;/p&gt;
&lt;p&gt;&lt;img alt="dense matrix." src="/images/dense.jpg" /&gt; &lt;/p&gt;
&lt;p&gt;Figure 3 : An example of a dense matrix.&lt;/p&gt;
&lt;p&gt;Sklearn represents dense data as  multidimensional numpy arrays.
   &lt;a href=""&gt;X's&lt;/a&gt; notes has a very thorough explanation of how this works.&lt;/p&gt;
&lt;h4&gt;3. Non standard textual data&lt;/h4&gt;
&lt;p&gt;For situations where models are to be built using text files or semi-stuctured python objects, sklearn attempts to &lt;code&gt;vectorize&lt;/code&gt; these files/objects into the more efficient numpy and scipy data types.&lt;/p&gt;
&lt;h2&gt;API Design Principles&lt;/h2&gt;
&lt;p&gt;In this section, I would discuss some of the design desicions that were made during the creation of the library, why they were made and how these decisions have influenced the way the library is 
 used now.&lt;/p&gt;
&lt;h3&gt;Consistency&lt;/h3&gt;
&lt;p&gt;The sklearn API was majorly designed to be consistent. This means object that do the same things are given a simple and consistent interface. Having a consistent API meant objects(sklearn objects)
 had to be grouped based on a number of conditions.
 The available types of sklearn objects are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; 1. Estimators and Meta-estimators
 2. Transformers
 3. Predictors
 4. Pipelines
 5. Feature Unions
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Estimators and meta-estimators:&lt;/p&gt;
&lt;p&gt;Estimator objects like you would expect are sklearn classes that estimate parameters in data. Both supervised and unsupervised sklearn algorithms provide support for estimators. Since estimator
objects do some form of &lt;code&gt;learning&lt;/code&gt;, the initialization and learning phase of estimators are separated.
If  &lt;code&gt;hyperparameter&lt;/code&gt; values are set during the initialization of estimators, the set values are used during learning. If not, the default values are used while learning.&lt;/p&gt;
&lt;p&gt;Once initialized, estimators expose a &lt;code&gt;fit&lt;/code&gt; method where the estimation learning happens. The &lt;code&gt;fit&lt;/code&gt; method is called with one parameter(or two if the task being handled is a 
supervised learning task. This parameter is the &lt;code&gt;label&lt;/code&gt; datasets.)  The &lt;code&gt;fit&lt;/code&gt; method when called learns by determining model-specific parameters from the training data and set these as attributes
 on the estimator object. Once learning is complete, this model sets its parameters(the learned parameters) gets exposed as class attributes with trailing underscore in their names(An example 
 is the &lt;code&gt;statistics_&lt;/code&gt; attribute of the &lt;code&gt;Imputer&lt;/code&gt; estimator.).&lt;/p&gt;
&lt;p&gt;In situations where a user needs to implement a custom estimator, the &lt;code&gt;BaseEstimator&lt;/code&gt; 
   class and it's(the required classifier's) 
   corresponding 
   &lt;code&gt;mixin&lt;/code&gt; can be 
   subclassed as shown below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.base&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BaseEstimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RegressorMixin&lt;/span&gt;

&lt;span class="c1"&gt;## A regressor mixin is being used here because &lt;/span&gt;
&lt;span class="c1"&gt;## the example is for a regressor Estimator.&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ExampleRegressorEstimator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEstimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RegressorMixin&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;An example of an regressor estimator. &lt;/span&gt;
&lt;span class="sd"&gt;Take a good look at docstring of the constructor, fit and predict methods. &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Called when initializing the classifier.&lt;/span&gt;
&lt;span class="sd"&gt;     Set default hyperparameter values here.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    This should fit classifier. All the &amp;quot;work&amp;quot; should be done here.&lt;/span&gt;

&lt;span class="sd"&gt;    Note: assert is not a good choice here and you should rather&lt;/span&gt;
&lt;span class="sd"&gt;    use try/except blog with exceptions.&lt;/span&gt;
&lt;span class="sd"&gt;     The y parameter must be set if the task is supervised learning.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Prediction Logic goes here. &lt;/span&gt;
&lt;span class="sd"&gt;    Remember to always check if the &lt;/span&gt;
&lt;span class="sd"&gt;    threshold attribute of your estimator is set. &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; This is only needed if the estimator is to be used with a GridSearchCV &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;set_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Meta estimators are a higher order estimators that are made from other (base) estimators. They require base estimators to be provided in their constructor. The most used example of 
   meta-estimators are ensemble methods available via &lt;code&gt;sklearn.ensemble&lt;/code&gt;. 
  In sklearn, each instance of a classifier (estimator) implements a corresponding meta-estimator. Custom meta-estimators can be created using the multiclass sklearn module and 
  implementing any of the &lt;a href="http://mlwiki.org/index.php/One-vs-All_Classification"&gt;one-vs-all&lt;/a&gt;, &lt;a href="https://www.quora.com/Whats-an-intuitive-explanation-of-one-versus-one-classification-for-support-vector-machines"&gt;one-vs-one&lt;/a&gt;
   or &lt;a href=""&gt;error correcting output&lt;/a&gt; algorithms.&lt;/p&gt;
&lt;p&gt;In conclusion, always keep the following at the back of your mind when writing or working with estimators and meta-estimators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Constructor parameters should have default values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Constructor shouldn't implement any form of logic(whether complex or not).
    Logic is best suited for the fit method.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;fit&lt;/code&gt; method should handle all logic and return an instance of the class.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transformers&lt;/p&gt;
&lt;p&gt;Transformer objects are somewhat &lt;code&gt;transmutational&lt;/code&gt; in nature. They exist to provide an interface for data modification or filtering within sklearn. Transformer objects mostly exists 
along sides estimator objects. This means some objects have both estimator and transformation features availble to them.&lt;/p&gt;
&lt;p&gt;Like estimators, transformers do not carry out any major logic in their constructor. Instead, they  expose a &lt;code&gt;transform&lt;/code&gt; method where the transformation happens. This method takes a dataset as 
a parameter. Transformers also have a &lt;code&gt;fit_transform&lt;/code&gt; or &lt;code&gt;fit_predict&lt;/code&gt;(fit_predict for clustering transformers) method that both &lt;code&gt;fits&lt;/code&gt; and &lt;code&gt;transforms&lt;/code&gt; data in one go. Preprocessing, feature 
selection, feature extraction and 
dimensionality 
reduction 
algorithms are all provided as transformers within the sklearn.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Imputer&lt;/span&gt;

&lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above block can be made better as shown below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;X_train = Imputer().fit(X_train).transform(X_train)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above line can even made more better as shown below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;X_train = Imputer().fit_transform(X_train)
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Predictors&lt;/p&gt;
&lt;p&gt;Predictor objects are able to make predictions. They're basically estimators with a &lt;code&gt;predict&lt;/code&gt; method. Predictors returns the predicted label as output. 
Asides prediction, some predictors expose methods to benchmark various aspects of the prediction. These methods can be any of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;decision_function&lt;/code&gt; for linear models to measure the distance between samples&lt;/li&gt;
&lt;li&gt;&lt;code&gt;score&lt;/code&gt; to  measure the quality of the prediction. This method computes the coefficient of determination  in regression and the accuracy in classification.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;predict_proba&lt;/code&gt; to measure class probabilities.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;predict_log_proba&lt;/code&gt; to measure log of class probabilities.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pipelines and Feature Unions&lt;/p&gt;
&lt;p&gt;Pipelines in machine learning are chaining tools. They are used to chain processes together and follow execution from one end of the chain to the other. Sklearn provides support for pipeline
objects that accept a list of transformers and a single estimator. &lt;/p&gt;
&lt;p&gt;The list of transformers handle intermediate steps(i.e feature extraction, dimensionality reduction, learning and making 
predictions) and (should) expose both a &lt;code&gt;fit&lt;/code&gt; and a &lt;code&gt;transform&lt;/code&gt; method. The estimator only has to implement a &lt;code&gt;fit&lt;/code&gt; method. Calling a pipeline, recursively fits then transforms the data set 
till it gets to the estimator then it calls the fit method of the estimator.&lt;/p&gt;
&lt;p&gt;&lt;img alt="dense matrix." src="/images/pipeline.png" /&gt; &lt;/p&gt;
&lt;p&gt;Figure 4: An example of a pipline diagram(Credits : &lt;a href="http://karlrosaen.com/ml/learning-log/2016-06-20/"&gt;http://karlrosaen.com/ml/learning-log/2016-06-20/&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The final usage(features) of a pipeline depends on the attributes of the estimator in the pipeline. This means, if the estimator is a predictor pipeline, the pipeline can itself be used as a 
predictor. 
If the estimator is a transformer, then the pipeline is a transformer pipeline. &lt;/p&gt;
&lt;p&gt;Feature Unions are quite similar to pipelines in the sense that they are also chaining tools. However unlike pipelines, feature unions only chain together transformation processes.
This means they accept a list of transformers as input and expose a fit method which recursively calls the base transformers &lt;code&gt;fit&lt;/code&gt; methods. The main advantage of feature union objects is that 
they can be used in place of transformers in pipelines.&lt;/p&gt;
&lt;p&gt;&lt;img alt="dense matrix." src="/images/pipelines.png" /&gt; &lt;/p&gt;
&lt;p&gt;Figure 5: Example pipeline to illustrate how feature unions work. This pipeline has 3 transformers and an estimator.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;PCA&lt;/code&gt; and &lt;code&gt;KernelPCA&lt;/code&gt; transformers in the pipeline image above can be combined into a single FeatureUnion which can then be passed down to the SelectKBest estimator and then to the logistic 
regression predictor.&lt;/p&gt;
&lt;p&gt;&lt;img alt="dense matrix." src="/images/feature_union.png" /&gt; &lt;/p&gt;
&lt;p&gt;Figure 6: An example of a feature union in a pipeline.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;FeatureUnion&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt; 
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;KernelPCA&lt;/span&gt; 
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SelectKBest&lt;/span&gt;

&lt;span class="n"&gt;union&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FeatureUnion&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pca&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;&lt;span class="p"&gt;()),(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;kpca&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;KernelPCA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

&lt;span class="n"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;feat union&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;union&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;feat sel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SelectKBest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;log reg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;penalty&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;l2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Other Principles&lt;/h2&gt;
&lt;p&gt;Asides consistency, a number of other principles the sklearn library tries to adhere by include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Inspection: This provides a reasonable interface to retrieve/inspect data from objects. For example, estimator hyperparameters are made to be directly accessible via public instance and their 
learnt parameters are also accessible via public instance variables with an underscore suffix &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Composition
    Blocks of code were placed together in an attempt to ensure reusability and reduce class proliferation. This, for example makes it easy to create a feature union from a sequence of transformers
     and to create a pipeline from a sequence of feature unions, transformers and an estimator without necessarily having to write new classes each time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Defaults:
    Sensible defaults are set in all the objects in sklearn. This provides a fallback mechanisms for variables/attributes to fall back to in case they're not set by the user.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Sklearn is a very interesting and powerful machine learning library. Given the fact that it covers most machine learning methods, sklearn can be a bit daunting to get through. 
   In this article, I have attempted to break down the major objects available in the library into simple understandable chunks that can be tackled at once. While, I did not cover every chunk in 
   complete detail, the resources section below provides a link to a number of articles/resources where more information can be gotten about them.&lt;/p&gt;
&lt;p&gt;If you do happen to have any question or comment, do feel free to leave one below or sync up with me on &lt;a href="https://twitter.com/__olamilekan__"&gt;twitter&lt;/a&gt;. &lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;https://github.com/scikit-learn/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/developers/index.html"&gt;http://scikit-learn.org/stable/developers/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/46691596/why-does-sklearn-imputer-need-to-fit"&gt;Why does sklearn Imputer need to fit?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scikit-learn-contrib/project-template/"&gt;https://github.com/scikit-learn-contrib/project-template/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/classes.html"&gt;http://scikit-learn.org/stable/modules/classes.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/ensemble.html"&gt;http://scikit-learn.org/stable/modules/ensemble.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/dev/developers/contributing.html#rolling-your-own-estimator"&gt;http://scikit-learn.org/dev/developers/contributing.html#rolling-your-own-estimator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"&gt;http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://karlrosaen.com/ml/learning-log/2016-06-20/"&gt;http://karlrosaen.com/ml/learning-log/2016-06-20/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1309.0238"&gt;https://arxiv.org/abs/1309.0238&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Olamilekan Wahab</dc:creator><pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:,2018-05-05:blog/2018/05/05/understanding-the-sklearn-design/</guid><category>machine-learning</category><category>sklearn</category><category>python</category></item><item><title>Locally Weighted Regression</title><link>/blog/2018/01/30/locally-weighted-regression/</link><description>&lt;p&gt;A couple of weeks back, I started a review of the linear models I've used over the years and and I realized that I never really understood how the locally weighted regression algorithm works. This and the fact that &lt;code&gt;sklearn&lt;/code&gt; had no support for it, encouraged me to do an investigation into the working principles of the algorithm. In this post, I would attempt to provide an overview of the algorithm using mathematical inference and list some of the implementations available in Python. &lt;/p&gt;
&lt;p&gt;The rest of this article will be organised as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression &lt;ul&gt;
&lt;li&gt;Regression Function&lt;/li&gt;
&lt;li&gt;Regression Assumptions&lt;/li&gt;
&lt;li&gt;The Linear Regression Algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Locally Weighted Regression&lt;ul&gt;
&lt;li&gt;Python Implementation&lt;ul&gt;
&lt;li&gt;StatsModel Implementation&lt;/li&gt;
&lt;li&gt;Alexandre Gramfort's implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Conclusion&lt;/li&gt;
&lt;li&gt;Resources&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Notations&lt;/h2&gt;
&lt;p&gt;The following notations would be used throughout this article&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symbol&lt;/th&gt;
&lt;th align="center"&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$$ y $$&lt;/td&gt;
&lt;td align="center"&gt;Target Variable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$ X $$&lt;/td&gt;
&lt;td align="center"&gt;Features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$ (X, y)   $$&lt;/td&gt;
&lt;td align="center"&gt;Training set&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$ n $$&lt;/td&gt;
&lt;td align="center"&gt;Number of features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$ X_i, y_i $$&lt;/td&gt;
&lt;td align="center"&gt;&lt;sup&gt;ith&lt;/sup&gt; index of X and y&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$ m $$&lt;/td&gt;
&lt;td align="center"&gt;Number of training examples&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Regression&lt;/h2&gt;
&lt;p&gt;Regression is the estimation of a continuous response variable based on the values of some other variable. The variable to be estimated is dependent on the other variable(s) in the function space. It is parametric in nature because it makes certain assumptions on the available data. If the data follows these &lt;a href="#regression-assumptions"&gt;assumptions&lt;/a&gt;, regression gives incredible results. Otherwise, it struggles to provide convincing accuracy. &lt;/p&gt;
&lt;h3&gt;Regression Assumptions&lt;/h3&gt;
&lt;p&gt;As was mentioned above, regression works best when the assumptions made about the available data are true. 
Some of these assumptions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There exists a linear relationship between $X$ and $y$. &lt;/p&gt;
&lt;p&gt;This assumes that a change in $X$ would lead to a corresponding change in $y$.
This assumptions is particularly important in linear regression of which locally weighted regression is a form.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There must be no correlation in the set of data in $X$. &lt;/p&gt;
&lt;p&gt;The presence of correlation in $X$ leads to a concept known as &lt;a href="https://en.wikipedia.org/wiki/Multicollinearity"&gt;multicollinearity&lt;/a&gt;. 
If variables are correlated, it becomes extremely difficult for the model to determine the true effect of $X$ on $Y$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Independence of errors&lt;/p&gt;
&lt;p&gt;This assumes that the errors of the response variables are uncorrelated with each other i.e the error at $ h(x)_i $ should not indicate the error at any other point. $h(x)_i$ is the estimate of the true function between Y and X. It's discussed in more context below.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$y$ must have a normal distribution to the error term&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Regression Function&lt;/h3&gt;
&lt;p&gt;The regession function is a parametric function used for estimating the target variable. This function can either be linear or non-linear. Now, since this article's main focus is the locally weighted regression which is a form of the linear regression, there would be a little more focus on linear regression.&lt;/p&gt;
&lt;p&gt;Linear regression is an approach for modelling the linear relationship between a scalar $y$ and a set of variables $X$.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Unfitted scatter plot." src="/images/scatter.png" /&gt; Figure 1&lt;/p&gt;
&lt;p&gt;Given a function whose scatter plot is above, a linear regression can be modeled to it by finding the line of best fit. Finding the &lt;strong&gt;line of best fit&lt;/strong&gt; in simple terms is really just getting the best function to represent the relationship between the $X$ and $y$ variables. This function, mostly called the &lt;code&gt;linear regression function&lt;/code&gt; or more generally the &lt;code&gt;hypothesis&lt;/code&gt; is a linear function which includes a dependent variable(target), a set of independent variables(features) and an unknown parameter. It's represented as:&lt;/p&gt;
&lt;p&gt;$$ h_{\theta}(x) = \theta_{0} + \theta_{1} X_{1} + \theta_{2} X_{2}+ ... + \theta_{n} X_{n} \label{a}\tag{1}$$&lt;/p&gt;
&lt;p&gt;When evaluated, $h_{\theta}(x)$ in $\ref{a}$ above becomes $h(x)$. The regression function is called &lt;code&gt;simple linear regression&lt;/code&gt; when only one independent variable $(X)$ is involved. In such cases, it becomes $$ h(x) = \theta_{0} + \theta_{1} X_{1} + \epsilon_{1} \label{b}\tag{2}$$ It's called &lt;code&gt;multivariate linear regression&lt;/code&gt; when there is more than a single value for $X$.&lt;/p&gt;
&lt;p&gt;Additionally, the equation above is the equation of a line, more formally represented as $y = mx + c$ . Given this, to simplify the function, the intercept $X_{0}$ at $\theta_{0}$,is assumed to be $1$ so that it becomes a summation equation expressed as:&lt;/p&gt;
&lt;p&gt;$$ h(x) = \sum_{i=0}^{n} \theta_{i} X_{i} + \epsilon_{i} \label{c}\tag{3}$$ &lt;/p&gt;
&lt;p&gt;An alternative representation of $\ref{c}$ when expressed in vector form is given as:&lt;/p&gt;
&lt;p&gt;$$ h(x) = Î¸^{{T}} x_{i} \label{d}\tag{4}$$ &lt;/p&gt;
&lt;h3&gt;The Linear Regression Algorithm&lt;/h3&gt;
&lt;p&gt;The linear regression algorithm applies the regression function in one form or another in predicting values using &lt;code&gt;real&lt;/code&gt; data.
Since this prediction can never really be totally accurate, an error (represented as $\epsilon$ in $\ref{b}$ above) is generated.
This error, formulated as $\epsilon = |y - h(x)|$ is the vertical distance from the actual $y$ value to our prediction ($h(x)$) for the same $x$.  The error has a direct relationship with the accuracy of the algorithm. This means the smaller the error, the higher the model accuracy and vice versa. As such, the algorithm attempts to minimize this error.     &lt;/p&gt;
&lt;p&gt;The process of minimizing the error involves selecting the most appropriate features($\theta$) to include in fitting the algorithm. 
A popular approach for selecting $\theta$'s is making $h(x)$ as close to to $y$ as possible for each item in $(X, y)$. To do this, a function caled the &lt;code&gt;cost function&lt;/code&gt; is used. The &lt;code&gt;cost function&lt;/code&gt; measures the closeness of each $h(x)$ to its corresponding $y$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The cost function calculates the &lt;code&gt;cost&lt;/code&gt; of your regression algorithm.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's represented mathematically as:&lt;/p&gt;
&lt;p&gt;$$ J(\theta) = (\frac{1}{2}) \sum_{i=1}^{m} \left| \left(h(x)_i - y_i \right)^2\right|  \label{e}\tag{5}$$&lt;/p&gt;
&lt;p&gt;So, essentially, the linear regression algorithm tries to choose the best $\theta$ to minimize $J(\theta)$ which would in turn reduce the measure of error.
To do this, the algorithm starts by :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Choosing a base value for $\theta$. &lt;/li&gt;
&lt;li&gt;Updating this value to make $J(\theta)$ smaller. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This goes on for many iterations until the value of $J(\theta)$ converges to it's local minima. An implementation of the above steps is called the &lt;code&gt;gradient descent&lt;/code&gt; algorithm. &lt;/p&gt;
&lt;p&gt;The working principles of the gradient descent are beyond the scope of this article and would be covered in a different article in the near future. Alternatively, a very good resource on how they work is available in Sebastian Ruder's paper &lt;a href="http://ruder.io/optimizing-gradient-descent/index.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In summary, to evaluate $h(x)$, i.e make a prediction, the linear regression algorithm:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Fits $\theta$ to minimize $\sum_{i}(y_i - \theta^T x_i)^2 \label{f}\tag{6}$. &lt;/p&gt;
&lt;p&gt;Upon successful fitting, the graph of the function above becomes
&lt;img alt="Fitted scatter plot." src="/images/fitted_scatter.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ouputs $\theta^T x$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Locally Weighted Regression&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Unfitted LWR." src="/images/unfitted_lwr.png" /&gt; Figure 3
&lt;img alt="Unfitted LWR." src="/images/fitted_lwr.png" /&gt;   Figure 4&lt;/p&gt;
&lt;p&gt;Compared to Figure 1, Figure 3 above, has a relatively higher number of mountains in the input/output relationship. Attempting to fit this with linear regression would result in getting a very high error and a line of best fit that does not optimally fit the data as shown in Figure 4. This error results from the fact that linear regression generally struggles in fitting functions with non-linear relationships. These difficulties introduce a new approach for fitting non-linear multivariate regression functions called "locally weighted regression".&lt;/p&gt;
&lt;p&gt;Locally weighted regression is a non-parametric variant of the linear regression for fitting data using multivariate smoothing. Often called &lt;code&gt;LOWESS&lt;/code&gt; (locally weighted scatterplot smoothing), this algorithm is a mix of multiple local regression models on a meta &lt;code&gt;k-nearest-neighor&lt;/code&gt;.
It's mostly used in cases where linear regression does not perform well i.e finds it very hard to find a line of best fit. &lt;/p&gt;
&lt;p&gt;It works by fitting simple models to localized subsets ,say $x$, of the data to build up a function that describes the deterministic part of the variation in the data. The points covered by each point (i.e neighorhood) $x$ is calculated using k-nearest-neighors.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For each selected $x_i$, LWR selects a point $x$ that acts as a neighorhood within which a local linear model is fitted.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LOWESS while fitting the data, gives more weight to points within the neighorhood of $x$ and less weight to points further away from it. A user dependent value, called the &lt;strong&gt;bandwidth&lt;/strong&gt; determines the size of the data to fit each local subset. 
The given weight $w$ at each point $i$ is calculated using:
$$w_i = \exp(- \frac{(x_i - x ) ^ 2}{2 \tau ^ 2}) \label{g}\tag{7}$$&lt;/p&gt;
&lt;p&gt;$w_i$ depends on the point $x_i$ at which $x$ is being calculated. Since, a small $|x_i âˆ’ x|$ yields a $w(i)$ close to 1 and a large  $|x_i âˆ’ x|$ yields a very small $w(i)$, the parameter ($\theta$) is calculated for the LOWESS by giving more weight to the points within the neighorhood of $x$ than the points outside it.&lt;/p&gt;
&lt;p&gt;Essentially, this algorithm makes a prediction by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Fitting $\theta$ to minimize $\sum_{i}w_i(y_i - \theta^T x_i)^2  \tag{8}$.       &lt;/p&gt;
&lt;p&gt;The fitting is done using either &lt;strong&gt;weighted linear least squares&lt;/strong&gt; or the &lt;strong&gt;weighted quadratic least squares&lt;/strong&gt;. The algorithm is called the LOESS when it's fitted using the &lt;strong&gt;weighted quadratic least square&lt;/strong&gt; regression.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ouputs $\theta^T x$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Python Implementation&lt;/h3&gt;
&lt;p&gt;The support for LOWESS in Python is rather poor. This is primarily because the algorithm is computationally intensive given that it has to fit $j$ number of lines at every point $x_i$ within the neighorhood of $x_i$.&lt;/p&gt;
&lt;p&gt;Regardless of this challenge, there are currently 2 implementations of the LOWESS algorithm in Python that I have come across. These are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Statsmodel Implementation
&lt;a href="http://www.statsmodels.org/devel/generated/statsmodels.nonparametric.smoothers_lowess.lowess.html"&gt;http://www.statsmodels.org/devel/generated/statsmodels.nonparametric.smoothers_lowess.lowess.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Statsmodel is a python package that provides a range of tools for carrying out statistical computation in Python.&lt;/p&gt;
&lt;p&gt;It provides support for LOWESS in it's &lt;code&gt;statsmodel.nonparametric&lt;/code&gt; module. Statsmodel supports&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A lowess function that outs smoothed estimates of endog at the given exog values from points (exog, endog)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;em&gt;exog&lt;/em&gt; and &lt;em&gt;endog&lt;/em&gt; expressions in the quote above represent 1-D numpy arrays.These arrays denote $x_i$ and $y_i$ from the equation&lt;/p&gt;
&lt;p&gt;This function takes input $y$ and $x$ and estimates each smooth $y_i$ closest to $(x_i, y_i)$ based on their values of $x$. It essentially uses a &lt;strong&gt;weighted linear least square&lt;/strong&gt; approach to fitting the data. 
A downside of this is that statsmodels combines fit and predict methods into one, and so doesn't allow prediction on new data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alexandre Gramfort's implementation&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/agramfort/850437"&gt;https://gist.github.com/agramfort/850437&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This implementation is quite similar to the statsmodel implementation in that it supports only 1-D numpy arrays.&lt;/p&gt;
&lt;p&gt;The function is:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;linalg&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;agramfort_lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;lowess(x, y, f=2./3., iter=3) -&amp;gt; yest&lt;/span&gt;

&lt;span class="sd"&gt;    Lowess smoother: Robust locally weighted regression.&lt;/span&gt;
&lt;span class="sd"&gt;    The lowess function fits a nonparametric regression curve to a scatterplot.&lt;/span&gt;
&lt;span class="sd"&gt;    The arrays x and y contain an equal number of elements; each pair&lt;/span&gt;
&lt;span class="sd"&gt;    (x[i], y[i]) defines a data point in the scatterplot. The function returns&lt;/span&gt;
&lt;span class="sd"&gt;    the estimated (smooth) values of y.&lt;/span&gt;

&lt;span class="sd"&gt;    The smoothing span is given by f. A larger value for f will result in a&lt;/span&gt;
&lt;span class="sd"&gt;    smoother curve. The number of robustifying iterations is given by iter. The&lt;/span&gt;
&lt;span class="sd"&gt;    function will run faster with a smaller number of iterations.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]))[&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="n"&gt;yest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;iteration&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
            &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]])&lt;/span&gt;
            &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;yest&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;residuals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yest&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residuals&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;6.0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;yest&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Benchmark&lt;/h3&gt;
&lt;p&gt;To benchmark the 3 implementations, let's declare the following constants:
1. Let $x$ be a set of 1000 random float between $-\tau$ and $\tau$.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;low&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;high&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Let $y$ be a function of the sine of x.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A scatter plot of the relationship between $x$ and $y$ is shown below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Unfitted LWR." src="/images/statsmodel_case.png" /&gt; Figure 3&lt;/p&gt;
&lt;p&gt;Now, predicting with lowess using :&lt;/p&gt;
&lt;p&gt;Statsmodel LOWESS:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api.nonparametric&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;lowess&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;frac&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;303&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Alexandre Gramfort's implementation&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;agramfort_lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;837&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Though the pure LOWESS functions are hardly used in Python, I hope I've been able to provide a little intuition into how they work and possible implementation. &lt;/p&gt;
&lt;p&gt;On why this is maths intensive, while I believe we can make-do with black-box implementations of fundamental tools constructed by our more algorithmically-minded colleagues, I am a firm believer that the more understanding we have about the low-level algorithms we're applying to our data, the better practitioners we'll be.&lt;/p&gt;
&lt;h3&gt;Resources&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Linear_regression"&gt;https://en.wikipedia.org/wiki/Linear_regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jeremykun.com/2013/08/18/linear-regression/"&gt;https://jeremykun.com/2013/08/18/linear-regression/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/26804656/why-do-we-use-gradient-descent-in-linear-regression"&gt;https://stackoverflow.com/questions/26804656/why-do-we-use-gradient-descent-in-linear-regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Regression_analysis#Regression_models"&gt;https://en.wikipedia.org/wiki/Regression_analysis#Regression_models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/In-what-situation-should-I-use-locally-weighted-linear-regression-when-I-do-predictions"&gt;https://www.quora.com/In-what-situation-should-I-use-locally-weighted-linear-regression-when-I-do-predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/Why-is-that-in-locally-weighted-learning-models-we-tend-to-use-linear-regression-and-not-non-linear-ones"&gt;https://www.quora.com/Why-is-that-in-locally-weighted-learning-models-we-tend-to-use-linear-regression-and-not-non-linear-ones&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Olamilekan Wahab</dc:creator><pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:,2018-01-30:blog/2018/01/30/locally-weighted-regression/</guid><category>Maths</category><category>Python</category><category>regression</category><category>lwr</category></item><item><title>Introduction To The Fast Fourier Transform In Python</title><link>/blog/2017/11/01/introduction-to-fourier-transform/</link><description>&lt;p&gt;NOTE : This is the first of a two part blog post on the topic.&lt;/p&gt;
&lt;p&gt;The Fast Fourier Transform is an algorithmic optimization of the Discrete Fourier Transform.&lt;/p&gt;
&lt;p&gt;I came across it a couple of weeks back and found it quite interesting because it's based on a topic I had previously done in class but never really got to use. In this post, I would give a brief explanation of the FFT algorithm stating its mathematical background. In the blogpost following this, various uses-cases of the FFT in Python would explored.&lt;/p&gt;
&lt;p&gt;The FFT algorithm is an implementation of the Discrete Fourier Transform which is a type of Fourier Transform. The other type is the  &lt;strong&gt;Inverse Discrete Fourier Transform&lt;/strong&gt;. It's an adaptation of the Fourier Transform, which in simple terms is an "attempt at digitizing the analog world".&lt;/p&gt;
&lt;h3&gt;The Fourier Transform&lt;/h3&gt;
&lt;p&gt;Everything can be described using a waveform. Waveforms are a function of time or any other variable(space for an example). The Fourier Transform provides a rather useful way of looking at these waveforms. 
In its basic form, a Fourier Transform breaks down a waveform into sinusoids.This siusoids go a long way into proving that waves are not made of discrete number of frequencies but rather of a continuous range of frequencies. &lt;/p&gt;
&lt;p&gt;In mathematical terms, a Fourier Transform converts a time-based signal(wave, filter etc) into a sum of its sine and cosine waves with varying amplitude and wavelength.&lt;/p&gt;
&lt;p&gt;For each frequency of wave in the signal, it assigns a complex-valued coefficient. This is called the Fourier coefficient.
The real part of this coefficient contains information about the waves' cosine amplitude and the imaginary part contains information about the waves' sine amplitude.&lt;/p&gt;
&lt;h3&gt;The Discrete Fourier Transform&lt;/h3&gt;
&lt;p&gt;The DFT is further divided into two.&lt;/p&gt;
&lt;p&gt;The Forward Transform represented as&lt;/p&gt;
&lt;p&gt;$$F(m)  = {1 \over N}\sum_{n=0}^{N-1} f(n) \cdot e^{-i2 \pi mn \over N} $$  &lt;/p&gt;
&lt;p&gt;The Backward/Inverse Transform represented as&lt;/p&gt;
&lt;p&gt;$$f(n)  = \sum_{m=0}^{N-1} F(m) \cdot e^{-i2 \pi mn \over N} $$        &lt;/p&gt;
&lt;p&gt;$ f(n) $ in both equations above is the value of the function $ f $ at point n. It can be either real or complex-valued.
$ F(m) $ on the other hand, is the coefficient for the $ m^{th} $ wave component and can only be complex. &lt;/p&gt;
&lt;p&gt;The movement from $ f(n) $ to $ F(m) $ defines a change in configuration from spacial to frequency based configurations. The Fourier coefficients at this point is represented as $v_m  = {m \over t_s N} $ where m=0, 1, 2, ..., ${N \over 2}$    for positive frequencies and as $v_m  = -{ (N - m + 1)\over t_s N} $ where m=${({N\over 2}) + 1}, {({N\over 2}) + 2}, {({N\over 2}) + 3}, ... , {({N\over 2})  + N}$ for negative frequencies.&lt;/p&gt;
&lt;h3&gt;Python Implementation&lt;/h3&gt;
&lt;p&gt;Given its relevance in so many areas, there are a lot of wrappers for computing the &lt;strong&gt;DFT&lt;/strong&gt; in Python. &lt;strong&gt;Numpy&lt;/strong&gt; has its &lt;code&gt;numpy.fft&lt;/code&gt; function and &lt;strong&gt;scipy&lt;/strong&gt; has its &lt;code&gt;scipy.fftpack&lt;/code&gt; implementation of this algorithm. &lt;/p&gt;
&lt;p&gt;Before looking at these use cases, let's attempt to write a dummy implementation of the &lt;strong&gt;DFT&lt;/strong&gt; in python.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt; 
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;cmath&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;dft_with_complex_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_array&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;input_array_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dft_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_array&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; 
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;number&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_array_length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
        &lt;span class="n"&gt;array_complex&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;complex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_array_length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;angle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2j&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;cmath&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;number&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;input_array_length&lt;/span&gt; 
            &lt;span class="n"&gt;array_complex&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;input_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;cmath&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dft_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;array_complex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;dft_matrix&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above function iterates through each item in the input data and manually assigns them to fields in the &lt;strong&gt;DFT&lt;/strong&gt; formula above. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The input in this case has to be an array of real values.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Checking the result by comparing it to numpy's FFT implementation, we get:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;allclose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dft_with_complex_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fft&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fft&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;True
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Looks like everything is working fine&lt;/p&gt;
&lt;p&gt;Timing the &lt;strong&gt;dft_with_complex_input&lt;/strong&gt; and the standard numpy implementation to view run_time differences, we get&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;standard_dft_with_complex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fft&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fft&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    1 loop, best of 3: 136 ms per loop
    The slowest run took 16.43 times longer than the fastest. This could mean that an intermediate result is being cached.
    100000 loops, best of 3: 11.7 Âµs per loop
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;From the time difference above, the &lt;strong&gt;&lt;strong&gt;dft_with_complex_input&lt;/strong&gt;&lt;/strong&gt; function is more than ten thousand times slower than the numpy implementation.&lt;/p&gt;
&lt;p&gt;Why?&lt;/p&gt;
&lt;p&gt;Its easy. &lt;/p&gt;
&lt;p&gt;To start with, the &lt;code&gt;dft_with_complex_input&lt;/code&gt; is a rather simple implementation involving loops.
Then it scales as an $O[N^2]$ whereas the standard numpy implementation scales as an  $O[N log N]$ .&lt;/p&gt;
&lt;p&gt;An alternative implementation of the &lt;strong&gt;DFT&lt;/strong&gt; is shown below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;dft_with_input_pair&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;real_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imaginary_input&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;real_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;imaginary_input&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;The lengths should be equal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;input_array_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;real_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imaginary_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inreal&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num_iterator_one&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_array_length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;real_sum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imaginary_sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num_iterator_two&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_array_length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  &lt;span class="c1"&gt;# For each input element&lt;/span&gt;
            &lt;span class="n"&gt;angle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_iterator_two&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_iterator_one&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;input_array_length&lt;/span&gt;
            &lt;span class="n"&gt;real_sum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt;  &lt;span class="n"&gt;real_input&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;imaginary_input&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;imaginary_sum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;real_input&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;imaginary_input&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;real_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;real_sum&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;imaginary_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;imaginary_sum&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outreal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;outimag&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;The above function uses a similar approach as the first one but uses a pair of real and imaginary input.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A third implementation of the &lt;strong&gt;DFT&lt;/strong&gt; in Python is shown below.
It uses optimised numpy array functions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;dft_with_numpy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;input_to_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;input_array_shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_to_array&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;input_array_range&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_array_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;input_array_rearrange&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;input_array_shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;output_matrix_vector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2j&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;input_array_rearrange&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;input_array_range&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;input_array_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_matrix_vector&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_to_array&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;N.B : You can go ahead and check the output of each function on your own and also compare run times with that of the standard numpy and scipy implementations of the &lt;strong&gt;DFT&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Scipy FFT&lt;/h3&gt;
&lt;p&gt;Scipy has an extensive support for the  Discrete Fourier Transform. It provides support for 3 types of FFTs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Standard FFTs&lt;/li&gt;
&lt;li&gt;Real FFTs&lt;/li&gt;
&lt;li&gt;Hermitian FFTs&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Numpy FFT&lt;/h3&gt;
&lt;p&gt;Like scipy, numpy has a very thorough and very documented support for the FFT. It provides support for FFT with 5 functions under the &lt;code&gt;numpy.fft&lt;/code&gt; module. These functions are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;numpy.fft.fft : This provides support for 1-Dimensional FFTs&lt;/li&gt;
&lt;li&gt;numpy.ifft2 : This provides support for inverse 2-Dimensional FFTs&lt;/li&gt;
&lt;li&gt;numpy.fftn : This provides support for n-D FFTs&lt;/li&gt;
&lt;li&gt;numpy.fftshift: This shifts zero-frequency terms to the center of the array. For two-dimensional input, swaps first and third quadrants, and second and fourth quadrants.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you are able to get to this point, you should have a clear idea of what the FFT algorithm is and how it can be implemented in various forms in Python.
In the accompanying post, I would look into each scipy and numpy fft function and give a detailed outline of their use-cases.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Olamilekan Wahab</dc:creator><pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:,2017-11-01:blog/2017/11/01/introduction-to-fourier-transform/</guid><category>Maths</category><category>Python</category><category>Fourier</category></item></channel></rss>